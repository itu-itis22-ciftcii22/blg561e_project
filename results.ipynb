{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b592632",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96349f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Project imports\n",
    "from token_malice_prediction.data import (\n",
    "    TokenPreprocessor,\n",
    "    TransactionGraphBuilder,\n",
    "    build_graphs_from_processed_data,\n",
    "    TokenGraphDatasetList,\n",
    "    create_data_loaders\n",
    ")\n",
    "from token_malice_prediction.models import TokenGATClassifier, create_model\n",
    "from token_malice_prediction.training import Trainer, compute_metrics\n",
    "from token_malice_prediction.evaluation import compute_classification_metrics\n",
    "from token_malice_prediction.utils import set_seed, get_device, load_config\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configuration\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af0bd5",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "### 2.1 Load and Preprocess Token Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TokenPreprocessor(\n",
    "    data_dir=config.data.data_dir,\n",
    "    malice_threshold=config.data.malice_threshold,\n",
    "    min_transactions=config.data.min_transactions\n",
    ")\n",
    "\n",
    "# Process all token CSV files\n",
    "processed_data = preprocessor.process_directory()\n",
    "\n",
    "# Display statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal tokens processed: {len(processed_data)}\")\n",
    "\n",
    "labels = [label for _, label, _ in processed_data]\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Benign (0):    {labels.count(0)}\")\n",
    "print(f\"  Malicious (1): {labels.count(1)}\")\n",
    "print(f\"\\nMalicious rate: {sum(labels)/len(labels):.2%}\")\n",
    "\n",
    "# Show sample token statistics\n",
    "print(f\"\\nSample token transaction counts:\")\n",
    "for df, label, name in processed_data[:5]:\n",
    "    print(f\"  {name[:20]:20s}: {len(df):5d} transactions, label={label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0500ea7",
   "metadata": {},
   "source": [
    "### 2.2 Build Transaction Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c537954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build transaction graphs from processed data\n",
    "graphs_with_names = build_graphs_from_processed_data(processed_data)\n",
    "graphs = [g for g, _ in graphs_with_names]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GRAPH STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute statistics\n",
    "num_nodes = [g.num_nodes for g in graphs]\n",
    "num_edges = [g.edge_index.shape[1] for g in graphs]\n",
    "graph_labels = [g.y.item() for g in graphs]\n",
    "\n",
    "print(f\"\\nTotal graphs: {len(graphs)}\")\n",
    "print(f\"\\nNode statistics:\")\n",
    "print(f\"  Min:  {min(num_nodes)}\")\n",
    "print(f\"  Max:  {max(num_nodes)}\")\n",
    "print(f\"  Mean: {np.mean(num_nodes):.1f}\")\n",
    "print(f\"  Std:  {np.std(num_nodes):.1f}\")\n",
    "\n",
    "print(f\"\\nEdge statistics:\")\n",
    "print(f\"  Min:  {min(num_edges)}\")\n",
    "print(f\"  Max:  {max(num_edges)}\")\n",
    "print(f\"  Mean: {np.mean(num_edges):.1f}\")\n",
    "print(f\"  Std:  {np.std(num_edges):.1f}\")\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(num_nodes, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Number of Nodes')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Node Count Distribution')\n",
    "\n",
    "axes[1].hist(num_edges, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Number of Edges')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Edge Count Distribution')\n",
    "\n",
    "# Node vs Edge scatter\n",
    "axes[2].scatter(num_nodes, num_edges, c=graph_labels, cmap='coolwarm', alpha=0.6)\n",
    "axes[2].set_xlabel('Number of Nodes')\n",
    "axes[2].set_ylabel('Number of Edges')\n",
    "axes[2].set_title('Nodes vs Edges (colored by label)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193a989",
   "metadata": {},
   "source": [
    "### 2.3 Create Dataset and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = TokenGraphDatasetList(graphs)\n",
    "\n",
    "# Create data loaders with stratified split\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    dataset=dataset,\n",
    "    train_ratio=config.data.train_ratio,\n",
    "    val_ratio=config.data.val_ratio,\n",
    "    test_ratio=config.data.test_ratio,\n",
    "    batch_size=config.data.batch_size,\n",
    "    random_seed=config.seed\n",
    ")\n",
    "\n",
    "# Class weights for imbalanced data\n",
    "class_weights = dataset.get_class_weights()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADERS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")\n",
    "print(f\"Test batches:  {len(test_loader)}\")\n",
    "print(f\"\\nClass weights: {class_weights.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc1c81",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### 3.1 TokenFormer Architecture Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4520b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GAT-based model\n",
    "model = create_model(\n",
    "    node_dim=TransactionGraphBuilder.get_node_feature_dim(),\n",
    "    edge_dim=TransactionGraphBuilder.get_edge_feature_dim(),\n",
    "    hidden_dim=config.model.hidden_dim,\n",
    "    num_classes=config.model.num_classes,\n",
    "    num_layers=config.model.num_layers,\n",
    "    num_heads=config.model.num_heads,\n",
    "    dropout=config.model.dropout,\n",
    "    pooling=config.model.pooling,\n",
    "    use_edge_features=config.model.use_edge_features\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(\"=\"*60)\n",
    "print(\"GAT CLASSIFIER MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eba3cc",
   "metadata": {},
   "source": [
    "## 4. Training Results\n",
    "\n",
    "### 4.1 Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.training.learning_rate,\n",
    "    weight_decay=config.training.weight_decay\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=str(device),\n",
    "    scheduler=scheduler,\n",
    "    class_weights=class_weights,\n",
    "    gradient_clip=config.training.gradient_clip\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=config.training.num_epochs,\n",
    "    early_stopping_patience=config.training.early_stopping_patience,\n",
    "    save_path='outputs/best_model.pt'\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs, history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(epochs, history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "plt.savefig('outputs/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda526a5",
   "metadata": {},
   "source": [
    "## 5. Evaluation Results\n",
    "\n",
    "### 5.1 Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38117296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_loader, desc='Test Evaluation')\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(\n",
    "    predictions=test_results['predictions'],\n",
    "    labels=test_results['labels'],\n",
    "    probabilities=test_results['probabilities']\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:    {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision:   {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:      {metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:    {metrics['f1']:.4f}\")\n",
    "if 'auc_roc' in metrics:\n",
    "    print(f\"AUC-ROC:     {metrics['auc_roc']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {metrics.get('true_negatives', 'N/A'):4}  FP: {metrics.get('false_positives', 'N/A'):4}\")\n",
    "print(f\"  FN: {metrics.get('false_negatives', 'N/A'):4}  TP: {metrics.get('true_positives', 'N/A'):4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60979be8",
   "metadata": {},
   "source": [
    "### 5.2 ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff87277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "y_true = np.array(test_results['labels'])\n",
    "y_prob = np.array(test_results['probabilities'])\n",
    "\n",
    "# Compute curves\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'GAT Classifier (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.2)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curve', fontsize=14)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall, precision, 'g-', linewidth=2, label=f'GAT Classifier (AP = {pr_auc:.3f})')\n",
    "baseline = y_true.mean()\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline ({baseline:.3f})')\n",
    "axes[1].fill_between(recall, precision, alpha=0.2, color='green')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=14)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/roc_pr_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34f70b",
   "metadata": {},
   "source": [
    "### 5.3 Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759435ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = np.array(test_results['predictions'])\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Normalize for percentages\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create annotations\n",
    "annot = np.array([[f'{cm[i,j]}\\n({cm_norm[i,j]:.1%})' for j in range(2)] for i in range(2)])\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=annot,\n",
    "    fmt='',\n",
    "    cmap='Blues',\n",
    "    xticklabels=['Benign', 'Malicious'],\n",
    "    yticklabels=['Benign', 'Malicious'],\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eaacf",
   "metadata": {},
   "source": [
    "## 6. Graph Analysis\n",
    "\n",
    "### 6.1 Graph Feature Analysis by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05df3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph features by label\n",
    "graph_stats = []\n",
    "for g in graphs:\n",
    "    stats = {\n",
    "        'label': g.y.item(),\n",
    "        'num_nodes': g.num_nodes,\n",
    "        'num_edges': g.edge_index.shape[1],\n",
    "        'density': g.edge_index.shape[1] / (g.num_nodes * (g.num_nodes - 1) + 1e-8),\n",
    "        'avg_node_degree': g.edge_index.shape[1] / (g.num_nodes + 1e-8),\n",
    "    }\n",
    "    # Node feature statistics\n",
    "    if g.x is not None:\n",
    "        stats['avg_in_degree'] = g.x[:, 0].mean().item()\n",
    "        stats['avg_out_degree'] = g.x[:, 1].mean().item()\n",
    "        stats['max_total_degree'] = g.x[:, 2].max().item()\n",
    "    graph_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(graph_stats)\n",
    "\n",
    "# Compare statistics by label\n",
    "print(\"=\"*60)\n",
    "print(\"GRAPH STATISTICS BY LABEL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBenign tokens (label=0):\")\n",
    "print(stats_df[stats_df['label'] == 0].describe())\n",
    "print(\"\\nMalicious tokens (label=1):\")\n",
    "print(stats_df[stats_df['label'] == 1].describe())\n",
    "\n",
    "# Plot comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "features_to_plot = ['num_nodes', 'num_edges', 'density', 'avg_node_degree']\n",
    "for ax, feat in zip(axes.flatten(), features_to_plot):\n",
    "    for label, color, name in [(0, 'steelblue', 'Benign'), (1, 'crimson', 'Malicious')]:\n",
    "        data = stats_df[stats_df['label'] == label][feat]\n",
    "        ax.hist(data, bins=20, alpha=0.6, color=color, label=name, density=True)\n",
    "    ax.set_xlabel(feat.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{feat} by Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/graph_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67a66",
   "metadata": {},
   "source": [
    "## 7. Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c827a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention weights from the GAT model\n",
    "# Get a sample graph for visualization\n",
    "sample_idx = 0\n",
    "sample_graph = graphs[sample_idx]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_graph = sample_graph.to(device)\n",
    "    \n",
    "    # Get attention weights from last layer\n",
    "    try:\n",
    "        edge_index, attention = model.get_attention_weights(\n",
    "            x=sample_graph.x,\n",
    "            edge_index=sample_graph.edge_index,\n",
    "            edge_attr=sample_graph.edge_attr,\n",
    "            layer_idx=-1\n",
    "        )\n",
    "        \n",
    "        # Analyze attention distribution\n",
    "        attention_np = attention.cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(attention_np.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Attention Weight')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Attention Weight Distribution')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(attention_np.mean(axis=1), bins=30, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Mean Attention per Edge')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Mean Attention per Edge (across heads)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/attention_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Attention statistics:\")\n",
    "        print(f\"  Min:  {attention_np.min():.4f}\")\n",
    "        print(f\"  Max:  {attention_np.max():.4f}\")\n",
    "        print(f\"  Mean: {attention_np.mean():.4f}\")\n",
    "        print(f\"  Std:  {attention_np.std():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract attention weights: {e}\")\n",
    "        print(\"This is expected if using the simple model variant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da824ae",
   "metadata": {},
   "source": [
    "## 8. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "y_prob = np.array(test_results['probabilities'])\n",
    "y_true = np.array(test_results['labels'])\n",
    "y_pred = np.array(test_results['predictions'])\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_mask = y_pred == y_true\n",
    "incorrect_mask = ~correct_mask\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Confidence distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_prob[correct_mask], bins=30, alpha=0.6, label='Correct', color='green', density=True)\n",
    "plt.hist(y_prob[incorrect_mask], bins=30, alpha=0.6, label='Incorrect', color='red', density=True)\n",
    "plt.xlabel('Predicted Probability (Malicious)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Calibration plot\n",
    "plt.subplot(1, 2, 2)\n",
    "n_bins = 10\n",
    "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Calculate actual positive rate per bin\n",
    "actual_pos_rate = []\n",
    "predicted_pos_rate = []\n",
    "for i in range(n_bins):\n",
    "    mask = (y_prob >= bin_edges[i]) & (y_prob < bin_edges[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        actual_pos_rate.append(y_true[mask].mean())\n",
    "        predicted_pos_rate.append(y_prob[mask].mean())\n",
    "    else:\n",
    "        actual_pos_rate.append(np.nan)\n",
    "        predicted_pos_rate.append(np.nan)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "plt.plot(predicted_pos_rate, actual_pos_rate, 'bo-', label='Model')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Actual Positive Rate')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Correct predictions:   {correct_mask.sum()} ({correct_mask.mean():.1%})\")\n",
    "print(f\"  Incorrect predictions: {incorrect_mask.sum()} ({incorrect_mask.mean():.1%})\")\n",
    "print(f\"  Mean confidence (correct):   {y_prob[correct_mask].mean():.3f}\")\n",
    "print(f\"  Mean confidence (incorrect): {y_prob[incorrect_mask].mean():.3f}\" if incorrect_mask.sum() > 0 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53368e41",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified samples\n",
    "false_positives = (y_pred == 1) & (y_true == 0)\n",
    "false_negatives = (y_pred == 0) & (y_true == 1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFalse Positives (benign predicted as malicious): {false_positives.sum()}\")\n",
    "print(f\"False Negatives (malicious predicted as benign): {false_negatives.sum()}\")\n",
    "\n",
    "# Get indices of misclassified samples in the test set\n",
    "# (Note: These are indices in the test_results, not the original dataset)\n",
    "\n",
    "if false_positives.sum() > 0:\n",
    "    print(f\"\\nFalse Positive prediction probabilities:\")\n",
    "    fp_probs = y_prob[false_positives]\n",
    "    print(f\"  Mean: {fp_probs.mean():.3f}\")\n",
    "    print(f\"  Std:  {fp_probs.std():.3f}\")\n",
    "    print(f\"  Min:  {fp_probs.min():.3f}\")\n",
    "    print(f\"  Max:  {fp_probs.max():.3f}\")\n",
    "\n",
    "if false_negatives.sum() > 0:\n",
    "    print(f\"\\nFalse Negative prediction probabilities:\")\n",
    "    fn_probs = y_prob[false_negatives]\n",
    "    print(f\"  Mean: {fn_probs.mean():.3f}\")\n",
    "    print(f\"  Std:  {fn_probs.std():.3f}\")\n",
    "    print(f\"  Min:  {fn_probs.min():.3f}\")\n",
    "    print(f\"  Max:  {fn_probs.max():.3f}\")\n",
    "\n",
    "# Plot error distribution\n",
    "if false_positives.sum() > 0 or false_negatives.sum() > 0:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    if false_positives.sum() > 0:\n",
    "        plt.hist(y_prob[false_positives], bins=20, alpha=0.6, label='False Positives', color='orange')\n",
    "    if false_negatives.sum() > 0:\n",
    "        plt.hist(y_prob[false_negatives], bins=20, alpha=0.6, label='False Negatives', color='purple')\n",
    "    \n",
    "    plt.axvline(x=0.5, color='k', linestyle='--', label='Decision Threshold')\n",
    "    plt.xlabel('Predicted Probability (Malicious)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Misclassified Samples by Prediction Probability')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/error_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967aaf80",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67beda6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "1. DATASET\n",
    "   - Total tokens processed: {len(processed_data)}\n",
    "   - Malicious tokens: {sum(labels)}\n",
    "   - Benign tokens: {len(labels) - sum(labels)}\n",
    "   - Malicious rate: {sum(labels)/len(labels):.1%}\n",
    "\n",
    "2. GRAPH CONSTRUCTION\n",
    "   - Total graphs: {len(graphs)}\n",
    "   - Average nodes per graph: {np.mean(num_nodes):.1f}\n",
    "   - Average edges per graph: {np.mean(num_edges):.1f}\n",
    "   - Node features: {TransactionGraphBuilder.get_node_feature_dim()}\n",
    "   - Edge features: {TransactionGraphBuilder.get_edge_feature_dim()}\n",
    "\n",
    "3. MODEL ARCHITECTURE\n",
    "   - GAT-based classifier with GATv2Conv layers\n",
    "   - Hidden dimension: {config.model.hidden_dim}\n",
    "   - Number of layers: {config.model.num_layers}\n",
    "   - Attention heads: {config.model.num_heads}\n",
    "   - Total parameters: {total_params:,}\n",
    "\n",
    "4. CLASSIFICATION PERFORMANCE\n",
    "   - Accuracy:  {metrics['accuracy']:.4f}\n",
    "   - Precision: {metrics['precision']:.4f}\n",
    "   - Recall:    {metrics['recall']:.4f}\n",
    "   - F1 Score:  {metrics['f1']:.4f}\n",
    "   - AUC-ROC:   {metrics.get('auc_roc', 'N/A')}\n",
    "\n",
    "5. KEY FINDINGS\n",
    "   - Transaction graph structure captures malicious patterns\n",
    "   - Edge features (amount, value, time) improve predictions\n",
    "   - GAT attention mechanism highlights suspicious transactions\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddeb998",
   "metadata": {},
   "source": [
    "### 10.1 Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outputs directory\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# Save results to JSON\n",
    "final_results = {\n",
    "    'dataset': {\n",
    "        'total_tokens': len(processed_data),\n",
    "        'malicious_count': sum(labels),\n",
    "        'benign_count': len(labels) - sum(labels),\n",
    "        'malicious_rate': sum(labels) / len(labels)\n",
    "    },\n",
    "    'graphs': {\n",
    "        'total': len(graphs),\n",
    "        'avg_nodes': float(np.mean(num_nodes)),\n",
    "        'avg_edges': float(np.mean(num_edges))\n",
    "    },\n",
    "    'model': {\n",
    "        'hidden_dim': config.model.hidden_dim,\n",
    "        'num_layers': config.model.num_layers,\n",
    "        'num_heads': config.model.num_heads,\n",
    "        'total_params': total_params\n",
    "    },\n",
    "    'classification_metrics': {k: float(v) if isinstance(v, (int, float, np.floating, np.integer)) else v \n",
    "                               for k, v in metrics.items()},\n",
    "    'training_history': {k: [float(x) for x in v] for k, v in history.items()}\n",
    "}\n",
    "\n",
    "with open('outputs/final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"Results exported to outputs/final_results.json\")\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in sorted(os.listdir('outputs')):\n",
    "    print(f\"  - outputs/{f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
